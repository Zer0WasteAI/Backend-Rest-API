# Plan de Optimizaci√≥n API - 50-100 Peticiones Concurrentes

## üìä An√°lisis Actual de la Arquitectura

### ‚úÖ **Fortalezas Identificadas**
- **Caching inteligente**: Redis + Flask-Caching implementado con estrategias espec√≠ficas por endpoint
- **Rate limiting**: Sistema configurado por tipos de endpoint (AI, CRUD, Auth)
- **Clean Architecture**: Separaci√≥n clara de responsabilidades
- **Optimizaci√≥n AI**: Cache espec√≠fico para operaciones costosas de IA (3600s para impacto ambiental)

### ‚ö†Ô∏è **Cuellos de Botella Identificados**

#### 1. **Base de Datos - CR√çTICO**
- **MySQL single instance** sin pooling optimizado
- **Conexiones s√≠ncronas** sin async/await
- **Sin read replicas** para distribuci√≥n de carga
- **Queries N+1** potenciales en relaciones ORM

#### 2. **Servidor de Aplicaciones - ALTO**
- **Flask development server** (no apto para producci√≥n)
- **Sin workers m√∫ltiples** 
- **Threads limitadas** por GIL de Python
- **Blocking I/O** en operaciones de base de datos

#### 3. **Concurrencia - ALTO**
- **No async/await** en endpoints cr√≠ticos
- **Operaciones AI s√≠ncronas** (Gemini, reconocimiento)
- **File uploads bloqueantes**

#### 4. **Infraestructura - MEDIO**
- **Container √∫nico** sin load balancer
- **Sin auto-scaling**
- **Memoria/CPU no optimizadas**

---

## üéØ Plan de Optimizaci√≥n - Orden de Prioridad

### **FASE 1: Quick Wins (1-2 d√≠as) - Mejora inmediata 2-3x**

#### 1.1 **Servidor de Producci√≥n - CR√çTICO**
```bash
# Reemplazar Flask dev server con Gunicorn + Nginx
pip install gunicorn gevent
```

**Implementaci√≥n:**
- **Gunicorn** con workers gevent (async)
- **Nginx** como reverse proxy y load balancer
- **Worker processes**: 2 * CPU cores + 1
- **Configuraci√≥n**: 4-6 workers para 50-100 requests

#### 1.2 **Database Connection Pooling - CR√çTICO**
```python
# Optimizar SQLAlchemy pool
SQLALCHEMY_ENGINE_OPTIONS = {
    'pool_size': 20,           # Conexiones permanentes
    'max_overflow': 40,        # Conexiones adicionales
    'pool_timeout': 30,        # Timeout esperando conexi√≥n
    'pool_recycle': 3600,      # Reciclar conexiones cada hora
    'pool_pre_ping': True      # Validar conexiones
}
```

#### 1.3 **Redis Performance Tuning - ALTO**
```python
# Optimizar configuraci√≥n Redis
REDIS_URL = 'redis://localhost:6379/0'
REDIS_CONNECTION_POOL = {
    'max_connections': 50,
    'retry_on_timeout': True,
    'socket_keepalive': True,
    'socket_keepalive_options': {}
}
```

### **FASE 2: Concurrencia (2-3 d√≠as) - Mejora 3-5x**

#### 2.1 **Async Endpoints Cr√≠ticos**
Convertir a async/await:
- `/api/recognition/*` (AI operations)
- `/api/inventory/bulk` (batch operations)  
- `/api/generation/*` (recipe generation)

```python
# Ejemplo implementaci√≥n
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
import asyncio
import asyncpg

@inventory_bp.route("/bulk-add", methods=["POST"])
@jwt_required()
async def add_bulk_ingredients():
    # Operaci√≥n async para 50-100 items simult√°neos
    await process_ingredients_async(ingredients_data)
```

#### 2.2 **Background Jobs para AI**
```python
# Celery + Redis para operaciones AI pesadas
from celery import Celery

celery = Celery('zerowasteai')
celery.conf.broker_url = 'redis://localhost:6379/1'

@celery.task
def process_ai_recognition_async(image_data, user_id):
    # Procesamiento AI en background
    result = gemini_adapter.analyze_image(image_data)
    cache_service.set(f"recognition:{user_id}", result, 3600)
    return result
```

### **FASE 3: Base de Datos (3-4 d√≠as) - Mejora 2-4x**

#### 3.1 **Query Optimization**
```python
# Eager loading para evitar N+1 queries
def get_user_inventory_optimized(user_id):
    return db.session.query(Inventory)\
        .options(joinedload(Inventory.ingredients))\
        .options(joinedload(Inventory.food_items))\
        .filter_by(user_id=user_id)\
        .all()

# √çndices espec√≠ficos
CREATE INDEX idx_inventory_user_expiration ON inventory(user_id, expiration_date);
CREATE INDEX idx_ingredients_name_type ON ingredients(name, storage_type);
```

#### 3.2 **Database Read Replicas**
```yaml
# docker-compose.yml - Agregar replica
mysql-replica:
  image: mysql:8.0
  command: --server-id=2 --log-bin=mysql-bin --binlog-do-db=zwaidb
  environment:
    MYSQL_MASTER_SERVICE: mysql
```

### **FASE 4: Infraestructura (2-3 d√≠as) - Mejora 2-3x**

#### 4.1 **Container Orchestration**
```yaml
# docker-compose-production.yml
version: '3.8'
services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - app1
      - app2

  app1: &app
    build: .
    environment:
      - WORKER_ID=1
    depends_on:
      - mysql
      - redis

  app2:
    <<: *app
    environment:
      - WORKER_ID=2

  redis:
    image: redis:7-alpine
    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru
```

#### 4.2 **Nginx Load Balancer**
```nginx
upstream backend {
    least_conn;
    server app1:3000 max_fails=3 fail_timeout=30s;
    server app2:3000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    
    location /api/ {
        proxy_pass http://backend;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_connect_timeout 30s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }
    
    # Cache est√°tico
    location /static/ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

---

## üìà **M√©tricas Esperadas de Performance**

### **Situaci√≥n Actual Estimada:**
- **Concurrencia**: 5-10 requests simult√°neas
- **Response time**: 200-500ms (endpoints simples), 2-5s (AI)
- **Throughput**: ~20 requests/segundo

### **Despu√©s de Optimizaciones:**

| Fase | Concurrencia | Response Time | Throughput | Mejora |
|------|-------------|---------------|------------|---------|
| **Fase 1** | 25-30 req | 100-200ms | 60 req/s | **3x** |
| **Fase 2** | 50-70 req | 80-150ms | 120 req/s | **6x** |
| **Fase 3** | 70-90 req | 50-100ms | 180 req/s | **9x** |
| **Fase 4** | 100+ req | 50-80ms | 250 req/s | **12x** |

---

## ‚ö° **Quick Actions - Implementar Hoy**

### 1. **Dockerfile Optimizado**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    gcc \
    default-libmysqlclient-dev \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Instalar servidor de producci√≥n
RUN pip install gunicorn gevent

COPY . .

ENV PYTHONUNBUFFERED=1
ENV FLASK_ENV=production

EXPOSE 3000

# Usar Gunicorn en lugar de Flask dev server
CMD ["gunicorn", "--bind", "0.0.0.0:3000", "--workers", "4", "--worker-class", "gevent", "--worker-connections", "1000", "src.main:app"]
```

### 2. **Database Config Mejorada**
```python
# src/config/config.py - Agregar
class ProductionConfig(Config):
    SQLALCHEMY_ENGINE_OPTIONS = {
        'pool_size': 20,
        'max_overflow': 40,  
        'pool_timeout': 30,
        'pool_recycle': 3600,
        'pool_pre_ping': True,
        'echo': False  # Desactivar debug SQL
    }
    
    # Redis optimizado
    REDIS_URL = os.getenv('REDIS_URL', 'redis://redis:6379/0')
    CACHE_REDIS_CONNECTION_POOL_KWARGS = {
        'max_connections': 50,
        'retry_on_timeout': True
    }
```

### 3. **Docker Compose Mejorado**
```yaml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    container_name: redis_cache
    command: redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    networks:
      - auth_net

  mysql:
    image: mysql:8.0
    container_name: mysql_db
    command: --innodb-buffer-pool-size=512M --max-connections=300
    environment:
      MYSQL_DATABASE: zwaidb
      MYSQL_ROOT_PASSWORD: rootpass
      MYSQL_USER: user
      MYSQL_PASSWORD: userpass
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - auth_net

  backend:
    build: .
    container_name: zerowasteai_api
    depends_on:
      - mysql
      - redis
    environment:
      FLASK_ENV: production
      REDIS_URL: redis://redis:6379/0
      DB_HOST: mysql_db
    networks:
      - auth_net
```

---

## üîß **Monitoreo y Testing**

### **Herramientas de Load Testing:**
```bash
# Apache Bench - Testing b√°sico
ab -n 1000 -c 50 http://localhost:3000/api/inventory/

# JMeter - Testing avanzado
# Configurar:
# - 50-100 threads (usuarios)
# - Ramp-up 30 segundos
# - Loop 10 veces
# - Assertions de response time < 500ms
```

### **M√©tricas a Monitorear:**
- **Response time percentiles** (P50, P95, P99)
- **Error rate** (< 1%)
- **Database connections** en uso
- **Redis hit ratio** (> 80%)
- **CPU/Memory usage** (< 70%)

---

## üí∞ **Costo vs Beneficio**

| Optimizaci√≥n | Esfuerzo | Costo | Impacto | ROI |
|--------------|----------|-------|---------|-----|
| **Gunicorn + Workers** | 2h | $0 | +200% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **DB Pool + Redis** | 4h | $0 | +150% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Async Endpoints** | 16h | $0 | +100% | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Load Balancer** | 8h | $10/mes | +80% | ‚≠ê‚≠ê‚≠ê‚≠ê |

**Inversi√≥n total**: ~30 horas desarrollo + $10/mes infraestructura  
**Resultado**: API capaz de manejar 100+ peticiones concurrentes

---

Este plan te permitir√° escalar de ~10 peticiones concurrentes actuales a 100+ peticiones con una inversi√≥n m√≠nima y mejoras incrementales probadas.